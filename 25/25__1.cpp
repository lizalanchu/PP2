// Подключаем необходимые библиотеки
#include <iostream>   // Для ввода/вывода данных
#include <vector>     // Для использования динамических массивов (векторов)
#include <mpi.h>      // Для параллельных вычислений с использованием MPI

// Главная функция программы
int main(int argc, char** argv) {
    // Объявляем переменные:
    int rank;       // Уникальный номер (ранг) текущего процесса
    int size;       // Общее количество процессов
    int n;          // Размер матриц (n x n)
    
    // Матрицы (хранятся как одномерные векторы):
    std::vector<double> A;  // Исходная матрица A
    std::vector<double> B;  // Исходная матрица B
    std::vector<double> C;  // Результирующая матрица C
    
    // Локальные части матриц для каждого процесса:
    std::vector<double> local_A;  // Часть матрицы A для текущего процесса
    std::vector<double> local_C;  // Часть результата для текущего процесса
    
    // Индексы для циклов:
    int i, j, k;

    // Инициализация MPI-окружения (должна быть первой MPI-операцией)
    MPI_Init(&argc, &argv);
    
    // Получаем номер текущего процесса (ранжирование процессов)
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    
    // Получаем общее количество процессов
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // Процесс с рангом 0 отвечает за ввод данных
    if (rank == 0) {
        // Запрос размера матриц у пользователя
        std::cout << "Введите размер матриц n: ";
        std::cin >> n;

        // Проверка, что размер матрицы кратен числу процессов
        if (n % size != 0) {
            std::cerr << "Ошибка: размер матрицы должен быть кратен количеству процессов!\n";
            // Аварийное завершение всех процессов
            MPI_Abort(MPI_COMM_WORLD, 1);
        }

        // Выделение памяти под матрицы (n*n элементов)
        A.resize(n * n);  // Матрица A в виде одномерного массива
        B.resize(n * n);  // Матрица B
        C.resize(n * n);  // Матрица для результатов

        // Ввод матрицы A построчно
        std::cout << "Введите элементы матрицы A (по строкам, через пробел):\n";
        for (i = 0; i < n * n; i++) {
            std::cin >> A[i];  // Чтение элемента матрицы
        }

        // Ввод матрицы B построчно
        std::cout << "Введите элементы матрицы B (по строкам, через пробел):\n";
        for (i = 0; i < n * n; i++) {
            std::cin >> B[i];  // Чтение элемента матрицы
        }
    }

    // Рассылка размера матрицы n всем процессам
    // MPI_Bcast - операция широковещательной рассылки
    MPI_Bcast(&n,          // Адрес отправляемых данных
              1,           // Количество элементов
              MPI_INT,     // Тип данных
              0,           // Ранг отправителя (процесс 0)
              MPI_COMM_WORLD);  // Коммуникатор (все процессы)

    // Дополнительная проверка кратности размера матрицы
    if (n % size != 0) {
        if (rank == 0) {
            std::cerr << "Ошибка: размер матрицы должен быть кратен количеству процессов!\n";
        }
        MPI_Finalize();  // Корректное завершение
        return 1;        // Выход с кодом ошибки
    }

    // Вычисление количества строк на каждый процесс
    int rows_per_process = n / size;
    
    // Выделение памяти под локальные части матриц
    local_A.resize(rows_per_process * n);  // Часть матрицы A
    local_C.resize(rows_per_process * n);  // Часть результата

    // Разделение матрицы A между процессами
    // MPI_Scatter - распределяет данные от одного процесса всем остальным
    MPI_Scatter(A.data(),              // Адрес исходных данных (на отправителе)
                rows_per_process * n,  // Количество элементов каждому процессу
                MPI_DOUBLE,            // Тип данных
                local_A.data(),        // Адрес буфера приема
                rows_per_process * n,  // Количество принимаемых элементов
                MPI_DOUBLE,            // Тип данных
                0,                     // Ранг отправителя (процесс 0)
                MPI_COMM_WORLD);       // Коммуникатор

    // Рассылка матрицы B всем процессам
    // На процессах, кроме 0, нужно сначала выделить память
    if (rank != 0) {
        B.resize(n * n);  // Выделение памяти под матрицу B
    }
    
    // MPI_Bcast рассылает данные всем процессам
    MPI_Bcast(B.data(),    // Адрес данных
              n * n,       // Количество элементов
              MPI_DOUBLE,  // Тип данных
              0,           // Ранг отправителя
              MPI_COMM_WORLD);

    // Параллельное умножение матриц - каждый процесс вычисляет свою часть
    for (i = 0; i < rows_per_process; i++) {      // По строкам локальной части
        for (j = 0; j < n; j++) {                // По столбцам матрицы B
            local_C[i * n + j] = 0.0;            // Обнуляем элемент результата
            for (k = 0; k < n; k++) {            // Вычисление скалярного произведения
                local_C[i * n + j] += local_A[i * n + k] * B[k * n + j];
            }
        }
    }

    // Сбор результатов на процессе 0
    // MPI_Gather собирает данные со всех процессов
    MPI_Gather(local_C.data(),        // Адрес отправляемых данных
               rows_per_process * n,  // Количество элементов
               MPI_DOUBLE,           // Тип данных
               C.data(),             // Адрес буфера приема (на процессе 0)
               rows_per_process * n,  // Количество элементов от каждого
               MPI_DOUBLE,           // Тип данных
               0,                    // Ранг получателя (процесс 0)
               MPI_COMM_WORLD);      // Коммуникатор

    // Вывод результата только на процессе 0
    if (rank == 0) {
        std::cout << "Результат умножения матриц C:\n";
        for (i = 0; i < n; i++) {          // По строкам
            for (j = 0; j < n; j++) {      // По столбцам
                std::cout << C[i * n + j] << "\t";  // Вывод с табуляцией
            }
            std::cout << "\n";  // Переход на новую строку
        }
    }

    // Завершение работы с MPI (обязательно для всех процессов)
    MPI_Finalize();
    
    // Успешное завершение программы
    return 0;
}